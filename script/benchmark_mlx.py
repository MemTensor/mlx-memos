import requests
import time
import json
import random
import statistics
import concurrent.futures
import string
import os

# ================= é…ç½®åŒºåŸŸ =================
# æœåŠ¡åœ°å€
API_URL = "http://127.0.0.1:8080/v1/chat/completions"
# æ¨¡å‹è·¯å¾„ï¼ˆéœ€ä¸å¯åŠ¨æœåŠ¡æ—¶ä¸€è‡´ï¼‰
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_PATH = os.path.abspath(os.path.join(SCRIPT_DIR, "../models/Qwen3-14B-MLX"))

# å‹æµ‹å‚æ•°
INPUT_TOKENS_TARGET = 5000  # ç›®æ ‡è¾“å…¥ Token é•¿åº¦
OUTPUT_TOKENS_TARGET = 3000 # ç›®æ ‡è¾“å‡º Token é•¿åº¦ (max_tokens)
TOTAL_REQUESTS = 20         # æ¯ä¸ªå¹¶å‘å±‚çº§çš„æ€»è¯·æ±‚æ•°
CONCURRENCY_LEVELS = [1, 3] # å¹¶å‘å±‚çº§åˆ—è¡¨

# éšæœºè¯åº“ï¼ˆç”¨äºç”Ÿæˆ Promptï¼‰
WORDS = ["apple", "banana", "cherry", "date", "elderberry", "fig", "grape", "honeydew", 
         "kiwi", "lemon", "mango", "nectarine", "orange", "papaya", "quince", "raspberry", 
         "strawberry", "tangerine", "ugli", "vanilla", "watermelon", "xigua", "yam", "zucchini",
         "run", "jump", "walk", "sleep", "eat", "drink", "think", "code", "debug", "deploy",
         "fast", "slow", "hard", "easy", "complex", "simple", "red", "green", "blue", "yellow"]

def generate_random_prompt(target_tokens):
    """
    ç”Ÿæˆè¿‘ä¼¼æŒ‡å®š token æ•°é‡çš„éšæœºæ–‡æœ¬ã€‚
    å¯¹äºå¤§å¤šæ•° Tokenizerï¼Œè‹±æ–‡å•è¯ + ç©ºæ ¼é€šå¸¸çº¦ä¸º 1-1.3 tokensã€‚
    è¿™é‡Œç®€å•æŒ‰ 1 word â‰ˆ 1 token ä¼°ç®—ã€‚
    """
    # ä¸ºäº†æ€§èƒ½ï¼Œå…ˆç”Ÿæˆä¸€ä¸ªè¾ƒé•¿çš„åŸºç¡€å—ï¼Œç„¶åé‡å¤
    base_len = 100
    base_words = [random.choice(WORDS) for _ in range(base_len)]
    
    total_words_needed = target_tokens
    repeats = total_words_needed // base_len
    remainder = total_words_needed % base_len
    
    words = []
    for _ in range(repeats):
        words.extend(base_words)
    words.extend([random.choice(WORDS) for _ in range(remainder)])
    
    return " ".join(words)

# é¢„ç”Ÿæˆä¸€ä¸ª Prompt ä»¥ä¿è¯æ‰€æœ‰è¯·æ±‚è¾“å…¥ä¸€è‡´ï¼ˆæ§åˆ¶å˜é‡ï¼‰ï¼Œä¹Ÿå¯ä»¥æ”¹ä¸ºæ¯æ¬¡éšæœº
print(f"ğŸ”„ Generating random prompt with ~{INPUT_TOKENS_TARGET} tokens...")
SHARED_PROMPT = generate_random_prompt(INPUT_TOKENS_TARGET)
print(f"âœ… Prompt ready. Length in chars: {len(SHARED_PROMPT)}")

def calculate_percentiles(data):
    """è®¡ç®— P50, P95, P99 ç­‰ç»Ÿè®¡å€¼"""
    if not data:
        return {k: 0 for k in ["mean", "min", "max", "p50", "p95", "p99"]}
    
    data.sort()
    n = len(data)
    
    def get_p(p):
        idx = int(n * p)
        return data[min(idx, n - 1)]

    return {
        "mean": statistics.mean(data),
        "min": data[0],
        "max": data[-1],
        "p50": get_p(0.50),  # Median
        "p95": get_p(0.95),
        "p99": get_p(0.99)
    }

def send_request(req_id):
    """å‘é€å•ä¸ªè¯·æ±‚å¹¶è®°å½•è¯¦ç»†æŒ‡æ ‡"""
    payload = {
        "model": MODEL_PATH,
        "messages": [{"role": "user", "content": SHARED_PROMPT}],
        "temperature": 0.7,
        "max_tokens": OUTPUT_TOKENS_TARGET,
        "stream": True 
    }
    
    start_time = time.time()
    first_token_time = None
    last_token_time = None
    token_count = 0
    
    try:
        with requests.post(API_URL, json=payload, stream=True) as response:
            if response.status_code != 200:
                return {"error": f"Status {response.status_code}"}

            for line in response.iter_lines():
                if not line: continue
                decoded_line = line.decode('utf-8')
                if not decoded_line.startswith('data: '): continue
                
                data_str = decoded_line[6:].strip()
                if data_str == '[DONE]': break
                
                try:
                    data = json.loads(data_str)
                    delta = data['choices'][0]['delta']
                    if 'content' in delta and delta['content']:
                        curr_t = time.time()
                        if first_token_time is None:
                            first_token_time = curr_t
                        last_token_time = curr_t
                        token_count += 1
                except:
                    continue

        end_time = time.time()
        
        # è®¡ç®—æ ¸å¿ƒæŒ‡æ ‡
        ttft = (first_token_time - start_time) if first_token_time else 0
        latency = end_time - start_time
        
        # Inter-Token Latency (æ’é™¤é¦–å­—)
        itl = 0
        if token_count > 1 and last_token_time and first_token_time:
            itl = (last_token_time - first_token_time) / (token_count - 1)
            
        # Tokens Per Second (é’ˆå¯¹è¯¥è¯·æ±‚)
        tps = token_count / latency if latency > 0 else 0

        return {
            "req_id": req_id,
            "success": True,
            "ttft": ttft,
            "itl": itl,
            "latency": latency,
            "tokens": token_count,
            "tps": tps
        }

    except Exception as e:
        return {"error": str(e)}

def print_stats_table(name, data, unit="s"):
    """æ‰“å°æ ¼å¼åŒ–çš„ç»Ÿè®¡è¡¨æ ¼"""
    stats = calculate_percentiles(data)
    print(f"   {name:<15} | Mean: {stats['mean']:6.4f}{unit} | Min: {stats['min']:6.4f}{unit} | Max: {stats['max']:6.4f}{unit} | P50: {stats['p50']:6.4f}{unit} | P95: {stats['p95']:6.4f}{unit} | P99: {stats['p99']:6.4f}{unit}")

def run_benchmark(concurrency):
    print(f"\n{'='*80}")
    print(f"ğŸš€ Starting Benchmark | Concurrency: {concurrency} | Total Requests: {TOTAL_REQUESTS}")
    print(f"{'='*80}")
    
    results = []
    start_wall_time = time.time()
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as executor:
        futures = [executor.submit(send_request, i) for i in range(TOTAL_REQUESTS)]
        
        for future in concurrent.futures.as_completed(futures):
            res = future.result()
            if "error" in res:
                print(f"âŒ Request failed: {res['error']}")
            else:
                results.append(res)
                # å®æ—¶ç®€æŠ¥ï¼šReq ID | TTFT | Tokens
                print(f"   [Req {res['req_id']:02d}] TTFT: {res['ttft']:.3f}s | Tokens: {res['tokens']} | ITL: {res['itl']:.3f}s | TPS: {res['tps']:.1f}")

    end_wall_time = time.time()
    total_wall_time = end_wall_time - start_wall_time
    
    if not results:
        print("âŒ No successful requests.")
        return

    # èšåˆæ•°æ®
    ttfts = [r['ttft'] for r in results]
    itls = [r['itl'] for r in results]
    latencies = [r['latency'] for r in results]
    req_tpss = [r['tps'] for r in results]
    total_tokens = sum(r['tokens'] for r in results)
    
    # ç³»ç»Ÿæ€»åå (Tokens / Wall Time)
    system_tps = total_tokens / total_wall_time
    # ç³»ç»Ÿ QPS (Requests / Wall Time)
    system_qps = len(results) / total_wall_time

    print(f"\nğŸ“Š Detailed Statistics (Concurrency {concurrency})")
    print("-" * 80)
    
    print_stats_table("TTFT", ttfts, "s")
    print_stats_table("ITL", itls, "s")
    print_stats_table("Latency", latencies, "s")
    print_stats_table("Req TPS", req_tpss, "")
    
    print("-" * 80)
    print(f"   Total Generated Tokens : {total_tokens}")
    print(f"   Total Wall Time        : {total_wall_time:.2f} s")
    print(f"   System QPS             : {system_qps:.2f} req/s")
    print(f"   System Throughput      : {system_tps:.2f} tokens/s")
    print("-" * 80)

if __name__ == "__main__":
    for c in CONCURRENCY_LEVELS:
        run_benchmark(c)
        time.sleep(3)
